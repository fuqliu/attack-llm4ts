<!DOCTYPE html>
<html>
<head>
  <title>attack LLM4TS</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Adversarial Vulnerabilities in Large Language Models for Time Series Forecasting</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Fuqiang_Liu2" target="_blank">Fuqiang Liu</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://openreview.net/profile?id=~Sicong_Jiang1" target="_blank">Sicong Jiang</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.mcgill.ca/civil/luis-miranda-moreno" target="_blank">Luis Miranda-Moreno</a>,</span>
                    <span class="author-block">
                      <a href="https://cse.umn.edu/dsi/seongjin-choi" target="_blank">Seongjin Choi</a>,</span>
                      <span class="author-block">
                        <a href="https://lijunsun.github.io/" target="_blank">Lijun Sun</a>,</span>
                      </span>
                      </div>

                  <div class="is-size-5 publication-authors">
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                    <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2412.08099" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/JohnsonJiang1996/AdvAttack_LLM4TS" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.08099" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-centered">
          <img src="static/images/Attack1.jpg" alt="Abstract Image" style="max-width: 100%; height: auto; margin-bottom: 20px;">
          <p>
          We manipulate LLM-based time series forecasting.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!--Motivation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <ul>
            <li>Time series forecasting plays a crucial role in informed decision-making across various domains;</li>
            <li>Large Language Model (LLM)-based forecasting models (e.g., <a href="https://arxiv.org/pdf/2310.03589" target="_blank"><u>TimeGPT</u></a>, <a href="https://arxiv.org/pdf/2310.07820" target="_blank"><u>LLMTime</u></a>, <a href="https://arxiv.org/pdf/2310.01728" target="_blank"><u>Time-LLM</u></a>) have demonstrated promising performance in various time series forecasting applications; </li>
            <li><strong style="color: red;">Can the predictions of LLM-based time series forecasting models be trusted?</strong></li>
            <li>The vulnerabilities of LLM-based time series forecasting models remain poorly understood. Particularly, existing adversarial attacks cannot be directly applied to these models due to the following two challenges:</li>
            <ul>
              <li>The ground truth, representing the value at a future time step, is not available during the runtime of time series forecasting. Nonetheless, most existing adversarial attacks depend on the ground truth to compute gradients and generate perturbations;</li>
              <em style="color: green;">For example, consider a 5-minute-ahead stock value prediction. At 10:00 am, the ground truth corresponds to the stock value at 10:05 am, which is unavailable at 10:00 am for both forecasters and attackers. Typical attack methods, such as FGSM and PGD, rely on the ground truth to compute gradients and generate adversarial examples.</em>
              <li>LLMs must be treated as strict black-box systems due to the difficulty of accessing their internal workings and parameters.</li>
              <em style="color: green;">For example, consider attacking the commecial forecasting model, <a href="https://arxiv.org/pdf/2310.03589" target="_blank"><u>TimeGPT</u></a>. It is impossible for an attacker to access the model's parameters, making it infeasible to use these parameters to generate adversarial examples.</em>    
            </ul>
            <li>To address this gap, we propose a targeted gradient-free attack to evaluate the robustness of LLM-based forecasting models.</li>
          </ul>   
        </div>
      </div>
    </div>
  </div>
</section>
<!--End Motivation -->

<!--Threat Model -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Threat Model</h2>
        <div class="content has-text-justified">
          <p>
            The threat model defines the assumptions, capabilities, and objectives of the attacker attempting to manipulate LLM-based forecasting models.
          </p>
          <p>
            We begin with a brief introduction to time series forecasting. Let \( \mathbf{X}_t \in \mathbb{R}^d \) denote  \( d \)-dimensional time series at time \(t\). Given a sequence of recent \(T\) historical observations \( \mathbf{X}_{t-T+1:t} \), a forecasting model, \( f(\cdot) \), is employed to predict the future values for the subsequent \( \tau \) time steps. Assuming \( \hat{\mathbf{Y}}_{t+1:t+\tau} \) denotes the predicted future values and \( \mathbf{Y}_{t+1:t+\tau} \) represents the corresponding ground truth values, the prediction is formulated as:
            \[
              \hat{\mathbf{Y}}_{t+1:t+\tau} = f\left(\mathbf{X}_{t-T+1:t}\right),    
            \]
            where \( f(\cdot) \) is a function parameterized by a large language model.
          </p>
          <p>
            The attacker's objective is to deceive an LLM-based time series forecasting model into producing anomalous outputs that deviate significantly from both its normal predictions and the corresponding ground truth, through the introduction of imperceptible perturbations. This adversarial attack problem can be framed as an optimization task as follows:
            \[
              \begin{split}
                \max_{\rho_{t-T+1:t}}~& \mathcal{L}\left(f\left(\mathbf{X}_{t-T+1:t}+\boldsymbol{\rho}_{t-T+1:t} \right), \mathbf{Y}_{t+1:t+\tau}\right)\\
                \text{s.t.}~&\ \left\|\rho_i \right\|_p \le\epsilon, i\in\left[t-T+1,t\right],
              \end{split}
            \]
            where \( \mathbf{X}_{t-T+1:t} \) denotes the clean input, \( \mathbf{Y}_{t+1:t+\tau} \) denotes the true future values, and \( \boldsymbol{\rho}_{t-T+1:t} \) denotes the adversarial perturbations. The loss function \( \mathcal{L} \) quantifies the discrepancy between the model's output and the ground truth, while \( \epsilon \) constrains the magnitude of the perturbations under the \( \ell_p \)-norm, ensuring that the adversarial attack remains imperceptible.
          </p>
          <p>
            In practical applications, accessing the full set of detailed parameters of an LLM is typically infeasible, compelling the attacker to treat the target model as a black-box system. Moreover, the ground truth is unavailable during the runtime of forecasting. Additionally, obtaining the complete training dataset is impractical, leaving the attacker without access to this information. The attackerâ€™s capabilities can therefore be summarized as follows:
          </p>
          <ul>
            <li><strong>no access to the training data</strong></li>
            <li><strong>no access to internal information of the LLM-based forecasting model</strong></li>
            <li><strong>no access to ground truth</strong></li>
            <li><strong>the ability to query the target model</strong></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!--End Threat Model -->

<!--Methodology -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <p>
            Large Language Models have shown promising performance 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!--End Methodology -->

<!--Empirical Study -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Empirical Study</h2>
        <div class="content has-text-justified">
          <p>
            Large Language Models have shown promising performance 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!--End Empirical Study -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{liu2024adversarial,
        title={Adversarial Vulnerabilities in Large Language Models for Time Series Forecasting},
        author={Liu, Fuqiang and Jiang, Sicong and Miranda-Moreno, Luis and Choi, Seongjin and Sun, Lijun},
        journal={arXiv preprint arXiv:2412.08099},
        year={2024}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->




<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
